{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9608cd-5a61-432a-bb9f-cd8e11d4ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle1/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "WARNING: OMP_NUM_THREADS set to 16, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n",
      "/root/miniconda3/envs/paddle1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/paddle1/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "W0613 03:31:17.285213  9151 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.9, Driver API Version: 12.8, Runtime API Version: 11.8\n",
      "W0613 03:31:17.285980  9151 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading cached features from cache_features/train_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/val_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/test_features_cached.pkl\n",
      "train_dataset total samples: 11184\n",
      "val_dataset total samples: 1309\n",
      "test_dataset total samples: 1129\n",
      "({'qImg_feature': array([5.0892222e-01, 1.0130705e-01, 4.1554770e-01, ..., 1.5136348e-01,\n",
      "       7.5677846e-05, 2.2541411e-01], dtype=float32), 'qCap': '看到有人说 这老头说了句话 不是我退休了 要是没退休 你早就在牢里了 说是某地政法系统的前领导 正局级干部退休的 我想问这种人敢说出这种话 在职间到底', 'imgs_features': [array([0.6166205 , 0.32743877, 0.23644671, ..., 0.6098976 , 0.32837993,\n",
      "       0.09397861], dtype=float32), array([0.23688133, 0.73640347, 0.12396187, ..., 0.24109195, 0.10670266,\n",
      "       0.16786775], dtype=float32), array([0.30910894, 0.14095385, 0.31212616, ..., 0.0553519 , 0.00824548,\n",
      "       0.04955674], dtype=float32)], 'caption': ['Boston Orange  波士頓菊子: 朱学渊  - 為中國史學的實證化而努力', '新华每日电讯-微报纸-2021年11月19日', '新华每日电讯-微报纸-2022年01月28日']}, 3, 3)\n",
      "[Tensor(shape=[2], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [1, 0]), [['DR Congo Receives $40 million UN Aid to Help Tackle Ebola ...', '', '', '', '', '', '', '', '', '', ''], ['Video player loading', 'Close up of Canadian PM Justin Trudeau speaking.', 'U.S. warplane downs object over northern Canada, Prime Minister Trudeau says - Los Angeles Times', 'An F-22 fighter jet is pictured during a NATO exercise in Lask, Poland, in October. The same type of jet shot down an \"unidentified object\" over Canadian airspace on Saturday, Prime Minister Justin Trudeau said.', \"Trudeau says new mystery 'object' over Canada shot down by US jet | The Times of Israel\", 'U.S. Jet Shoots Down Another Unidentified Object Over Canada - The New York Times', \"The U.S. military shot down an unidentified object over Canada's Yukon territory : NPR\", 'Canadian Prime Minister Justin Trudeau speaks as he meets with US President Joe Biden at the InterContinental Presidente Mexico City hotel in Mexico City, January 10, 2023. (AP Photo/Andrew Harnik)', 'Canadian Prime Minister Justin Trudeau gesturing with his left hand as he speaks at a lectern', 'Canadian Prime Minister Justin Trudeau says teams searching for debris of object shot down over Yukon - ABC News', 'Canadian Prime Minister Justin Trudeau said his military would recovery the aircraft wreckage for study.']], Tensor(shape=[2, 5, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[[0.20231555, 0.20560628, 0.75229037, ..., 0.34712628,\n",
      "          0.24724418, 0.21719913],\n",
      "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "          0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "          0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "          0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "          0.        , 0.        ]],\n",
      "\n",
      "        [[0.07444888, 1.04318047, 0.08378710, ..., 0.19547231,\n",
      "          0.34330380, 0.61175835],\n",
      "         [0.19744906, 0.40876535, 0.04529139, ..., 0.21283349,\n",
      "          0.21853387, 0.50632828],\n",
      "         [0.18898675, 0.35962185, 0.04471598, ..., 0.19694205,\n",
      "          0.23298426, 0.50704706],\n",
      "         [0.03003026, 0.23254760, 0.11668351, ..., 0.96141303,\n",
      "          0.07227823, 1.12290168],\n",
      "         [0.18851021, 0.39503923, 0.03720920, ..., 0.20802671,\n",
      "          0.23960903, 0.50573635]]]), ['The end of the Ebola outbreak in Northeastern DRC is officially announced by DRC Health Minister LongondoEteni!', 'Canadian Prime Minister Justin Trudeau said an unidentified object has been shot down over Canadian airspace.'], Tensor(shape=[2, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[0.64731282, 0.46013868, 0.13194939, ..., 0.62589663, 0.20118716,\n",
      "         0.26323053],\n",
      "        [0.05225452, 0.11991781, 0.37704793, ..., 0.14654692, 0.17804545,\n",
      "         0.33930182]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-13 03:31:21,019] [    INFO]\u001b[0m - Loading weights file from cache at /root/.paddlenlp/models/ernie-m-base/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:31:22,367] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:31:23,787] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieMModel.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:31:23,789] [    INFO]\u001b[0m - All the weights of ErnieMModel were initialized from the model checkpoint at ernie-m-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieMModel for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:31:24,482] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-m-base/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:31:24,484] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/ernie-m-base/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 55920\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/5592 [00:00<?, ?it/s]W0613 03:31:24.920608  9151 gpu_resources.cc:306] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\n",
      "Training Epoch 1: 100%|█████████▉| 5590/5592 [09:30<00:00, 10.38it/s, loss=0.0114, acc=0.6763, step=5592] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.28057, acc: 0.70894, f1: 0.65595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5592/5592 [09:35<00:00,  9.71it/s, loss=0.0114, acc=0.6763, step=5592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 5592 | F1: 0.6559 (updated)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  80%|████████  | 4498/5592 [07:29<01:58,  9.26it/s, loss=0.0249, acc=0.7414, step=10090]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|█████████▉| 5590/5592 [09:34<00:00, 10.57it/s, loss=0.0006, acc=0.7437, step=11184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.16250, acc: 0.74484, f1: 0.71191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5592/5592 [09:39<00:00,  9.65it/s, loss=0.0006, acc=0.7437, step=11184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 11184 | F1: 0.7119 (updated)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  16%|█▋        | 912/5592 [01:31<07:25, 10.50it/s, loss=0.0029, acc=0.7862, step=12096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  53%|█████▎    | 2985/5592 [04:57<04:00, 10.84it/s, loss=0.0315, acc=0.7769, step=14169] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  75%|███████▍  | 4187/5592 [06:56<02:13, 10.55it/s, loss=0.0541, acc=0.7779, step=15372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5592/5592 [09:32<00:00,  2.61s/it, loss=0.6921, acc=0.7798, step=16776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.07662, acc: 0.73262, f1: 0.69506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5592/5592 [09:32<00:00,  9.77it/s, loss=0.6921, acc=0.7798, step=16776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  26%|██▋       | 1476/5592 [02:28<06:48, 10.09it/s, loss=0.2578, acc=0.8226, step=18253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  98%|█████████▊| 5487/5592 [09:09<00:10, 10.47it/s, loss=4.2655, acc=0.8089, step=22264] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|█████████▉| 5591/5592 [09:32<00:00, 10.21it/s, loss=0.0122, acc=0.8095, step=22368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.02347, acc: 0.75630, f1: 0.73193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5592/5592 [09:40<00:00,  3.79s/it, loss=0.0122, acc=0.8095, step=22368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 22368 | F1: 0.7319 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5592/5592 [09:41<00:00,  9.62it/s, loss=0.0122, acc=0.8095, step=22368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  10%|█         | 576/5592 [00:58<08:09, 10.24it/s, loss=0.0235, acc=0.8438, step=22944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  47%|████▋     | 2628/5592 [04:23<04:45, 10.37it/s, loss=0.0001, acc=0.8400, step=24996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  84%|████████▍ | 4687/5592 [07:49<01:26, 10.44it/s, loss=0.0475, acc=0.8435, step=27055] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|█████████▉| 5591/5592 [09:31<00:00, 10.06it/s, loss=0.3506, acc=0.8425, step=27960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.21259, acc: 0.75936, f1: 0.74050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5592/5592 [09:40<00:00,  3.86s/it, loss=0.3506, acc=0.8425, step=27960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 27960 | F1: 0.7405 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5592/5592 [09:41<00:00,  9.62it/s, loss=0.3506, acc=0.8425, step=27960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  20%|█▉        | 1102/5592 [01:50<07:36,  9.84it/s, loss=0.0000, acc=0.8866, step=29062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  26%|██▌       | 1446/5592 [02:25<06:27, 10.70it/s, loss=0.0001, acc=0.8808, step=29407] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  97%|█████████▋| 5451/5592 [09:05<00:13, 10.39it/s, loss=0.0102, acc=0.8781, step=33412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|█████████▉| 5591/5592 [09:20<00:00,  9.05it/s, loss=0.0133, acc=0.8785, step=33552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.20564, acc: 0.77540, f1: 0.75805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5592/5592 [09:42<00:00,  6.48s/it, loss=0.0133, acc=0.8785, step=33552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 33552 | F1: 0.7580 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5592/5592 [09:42<00:00,  9.60it/s, loss=0.0133, acc=0.8785, step=33552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  33%|███▎      | 1866/5592 [03:06<06:45,  9.19it/s, loss=0.0000, acc=0.9097, step=35418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  42%|████▏     | 2352/5592 [03:53<05:25,  9.96it/s, loss=0.0000, acc=0.9077, step=35904]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5592/5592 [09:34<00:00,  3.24s/it, loss=3.0182, acc=0.9102, step=39144] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.47559, acc: 0.76776, f1: 0.75210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5592/5592 [09:35<00:00,  9.72it/s, loss=3.0182, acc=0.9102, step=39144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  14%|█▍        | 791/5592 [01:18<07:42, 10.39it/s, loss=0.0000, acc=0.9254, step=39935] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  18%|█▊        | 983/5592 [01:38<07:46,  9.89it/s, loss=0.0000, acc=0.9273, step=40127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  63%|██████▎   | 3499/5592 [05:49<03:21, 10.39it/s, loss=0.0000, acc=0.9320, step=42643] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|█████████▉| 5578/5592 [09:19<00:01, 10.61it/s, loss=0.0041, acc=0.9336, step=44722] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5592/5592 [09:38<00:00,  4.30s/it, loss=0.0000, acc=0.9336, step=44736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.60233, acc: 0.77158, f1: 0.75148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5592/5592 [09:38<00:00,  9.66it/s, loss=0.0000, acc=0.9336, step=44736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  39%|███▉      | 2189/5592 [03:40<05:22, 10.54it/s, loss=0.0000, acc=0.9559, step=46926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  82%|████████▏ | 4598/5592 [07:42<01:36, 10.35it/s, loss=1.3808, acc=0.9530, step=49335] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|█████████▉| 5591/5592 [09:32<00:00, 10.57it/s, loss=5.7431, acc=0.9534, step=50328] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.61498, acc: 0.77998, f1: 0.75909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5592/5592 [09:42<00:00,  3.81s/it, loss=5.7431, acc=0.9534, step=50328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 50328 | F1: 0.7591 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5592/5592 [09:43<00:00,  9.59it/s, loss=5.7431, acc=0.9534, step=50328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  24%|██▍       | 1330/5592 [02:12<06:49, 10.42it/s, loss=0.0051, acc=0.9617, step=51659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  60%|██████    | 3376/5592 [05:39<03:38, 10.16it/s, loss=0.0000, acc=0.9606, step=53704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  90%|█████████ | 5060/5592 [08:26<00:51, 10.24it/s, loss=0.0000, acc=0.9603, step=55388] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|█████████▉| 5591/5592 [09:29<00:00, 10.06it/s, loss=0.0000, acc=0.9599, step=55920] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.63478, acc: 0.77846, f1: 0.75975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5592/5592 [09:39<00:00,  3.80s/it, loss=0.0000, acc=0.9599, step=55920]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 55920 | F1: 0.7597 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5592/5592 [09:40<00:00,  9.63it/s, loss=0.0000, acc=0.9599, step=55920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1129/1129 [00:25<00:00, 44.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os\n",
    "import imghdr\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.optimizer import AdamW\n",
    "from paddle import nn\n",
    "\n",
    "\n",
    "# 更通用的写法，兼容 Jupyter 和脚本运行\n",
    "try:\n",
    "    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'autodl-tmp/queries_dataset_merge')\n",
    "\n",
    "paddle.set_device('gpu')  # 如果没 GPU，可以改为 'cpu'\n",
    "\n",
    "#读取数据\n",
    "import json\n",
    "data_items_train = json.load(open(os.path.join(dataset_dir, 'dataset_items_train.json'), encoding='utf-8'))\n",
    "data_items_val = json.load(open(os.path.join(dataset_dir, 'dataset_items_val.json'), encoding='utf-8'))\n",
    "data_items_test = json.load(open(os.path.join(dataset_dir, 'dataset_items_test.json'), encoding='utf-8'))\n",
    "\n",
    "\n",
    "#读取数据中的每一个样本：图像img、文本caption、\n",
    "#对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>', '')\n",
    "    input_str = input_str.replace('</b>', '')\n",
    "    # input_str = unidecode(input_str)\n",
    "    return input_str\n",
    "\n",
    "\n",
    "class FeatureCachedNewsContextDataset(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split, resnet_model, cache_dir='cache_features'):\n",
    "        self.cache_path = os.path.join(cache_dir, f'{split}_features_cached.pkl')\n",
    "        self.split = split\n",
    "        self.resnet = resnet_model\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(self.cache_path):\n",
    "            print(f\"[INFO] Loading cached features from {self.cache_path}\")\n",
    "            with open(self.cache_path, 'rb') as f:\n",
    "                self.samples = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"[INFO] Creating cache with CNN features for {split} set...\")\n",
    "            self.samples = self.preprocess_and_cache(context_data_items_dict, queries_root_dir)\n",
    "            with open(self.cache_path, 'wb') as f:\n",
    "                pickle.dump(self.samples, f)\n",
    "            print(f\"[INFO] Cached features saved to {self.cache_path}\")\n",
    "\n",
    "    def preprocess_and_cache(self, data_dict, queries_root_dir):\n",
    "        from PIL import Image\n",
    "        import imghdr\n",
    "        from paddle.vision import transforms as T\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        def load_image(image_path):\n",
    "            try:\n",
    "                if imghdr.what(image_path) == 'gif':\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                else:\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                return transform(img)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        def process_string(text):\n",
    "            return text.replace('&#39;', ' ').replace('<b>', '').replace('</b>', '')\n",
    "\n",
    "        def extract_captions(inv_dict, direct_dict):\n",
    "            captions = []\n",
    "            for key in ['all_fully_matched_captions', 'all_partially_matched_captions']:\n",
    "                for page in inv_dict.get(key, []):\n",
    "                    if 'title' in page:\n",
    "                        captions.append(process_string(page['title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            for key in ['images_with_captions', 'images_with_caption_matched_tags', 'images_with_no_captions']:\n",
    "                for page in direct_dict.get(key, []):\n",
    "                    if 'page_title' in page:\n",
    "                        captions.append(process_string(page['page_title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            return list(set(captions))\n",
    "\n",
    "        MAX_IMG_PER_SAMPLE = 100\n",
    "        samples = []\n",
    "\n",
    "        for key in tqdm(data_dict, desc=f\"Processing {self.split} with features\"):\n",
    "            item = data_dict[key]\n",
    "            try:\n",
    "                qimg_path = os.path.join(queries_root_dir, item['image_path'])\n",
    "                qimg_tensor = load_image(qimg_path)\n",
    "                if qimg_tensor is None:\n",
    "                    continue\n",
    "                qImg_feature = self.resnet(qimg_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "                direct_path = os.path.join(queries_root_dir, item['direct_path'])\n",
    "                inverse_path = os.path.join(queries_root_dir, item['inv_path'])\n",
    "\n",
    "                with open(os.path.join(direct_path, 'direct_annotation.json'), encoding='utf-8') as f:\n",
    "                    direct_dict = json.load(f)\n",
    "                with open(os.path.join(inverse_path, 'inverse_annotation.json'), encoding='utf-8') as f:\n",
    "                    inv_dict = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {e}, skipping {key}\")\n",
    "                continue\n",
    "\n",
    "            evidence_features = []\n",
    "            for key1 in ['images_with_captions', 'images_with_no_captions', 'images_with_caption_matched_tags']:\n",
    "                pages = direct_dict.get(key1, [])\n",
    "                for i, page in enumerate(pages):\n",
    "                    if i >= MAX_IMG_PER_SAMPLE:\n",
    "                        break\n",
    "                    img_path = os.path.join(direct_path, page['image_path'].split('/')[-1])\n",
    "                    img_tensor = load_image(img_path)\n",
    "                    if img_tensor is not None:\n",
    "                        img_feature = self.resnet(img_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "                        evidence_features.append(img_feature)\n",
    "\n",
    "            if len(evidence_features) == 0:\n",
    "                continue\n",
    "\n",
    "            captions = extract_captions(inv_dict, direct_dict)\n",
    "            sample = {\n",
    "                'qImg_feature': qImg_feature,\n",
    "                'qCap': item['caption'],\n",
    "                'imgs_features': evidence_features,\n",
    "                'caption': captions\n",
    "            }\n",
    "            if self.split != 'test':\n",
    "                sample['label'] = int(item['label'])\n",
    "\n",
    "            samples.append(sample)\n",
    "\n",
    "            # 显存清理\n",
    "            del qImg_feature, evidence_features, img_tensor, qimg_tensor\n",
    "            gc.collect()\n",
    "            paddle.device.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[INFO] Cached {len(samples)} samples for {self.split}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.split != 'test':\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "        else:\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "\n",
    "from paddle.vision import models\n",
    "from paddle import nn\n",
    "import paddle\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch='resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        elif resnet_arch == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet arch: {resnet_arch}\")\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0], out.shape[1]))\n",
    "        return out\n",
    "\n",
    "#### load Datasets ####\n",
    "resnet = EncoderCNN(resnet_arch='resnet50')\n",
    "resnet.eval()\n",
    "train_dataset = FeatureCachedNewsContextDataset(data_items_train, dataset_dir, 'train', resnet)\n",
    "val_dataset = FeatureCachedNewsContextDataset(data_items_val, dataset_dir, 'val', resnet)\n",
    "test_dataset = FeatureCachedNewsContextDataset(data_items_test, dataset_dir, 'test', resnet)\n",
    "print(f\"train_dataset total samples: {len(train_dataset)}\")\n",
    "print(f\"val_dataset total samples: {len(val_dataset)}\")\n",
    "print(f\"test_dataset total samples: {len(test_dataset)}\")\n",
    "\n",
    "# 打印数据\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "def collate_context_cached_train(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch, labels = [], [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "        labels.append(paddle.to_tensor(sample['label']))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "    labels = paddle.stack(labels)\n",
    "\n",
    "    return labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "def collate_context_cached_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch = [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "\n",
    "    return caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "\n",
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "                              collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False,\n",
    "                            collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                             collate_fn=collate_context_cached_test, return_list=True, num_workers=2)\n",
    "\n",
    "# 打印数据\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "#模型构建\n",
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.attention_text = nn.MultiHeadAttention(embed_dim=768, num_heads=16)\n",
    "        self.attention_image = nn.MultiHeadAttention(embed_dim=2048, num_heads=16)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768, 3)\n",
    "        else:\n",
    "            self.classifier1 = nn.Linear(2 * (768 + 2048), 1024)\n",
    "            self.classifier2 = nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, qCap, qImg_feature, caps, imgs_features):\n",
    "        # Encode qCap\n",
    "        encode_dict_qcap = self.tokenizer(text=qCap, max_length=128, truncation=True, padding='max_length')\n",
    "        input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "        qcap_feature, _ = self.ernie(input_ids_qcap)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:, 0, :])\n",
    "            return logits\n",
    "\n",
    "        # Encode all evidence captions\n",
    "        caps_feature = []\n",
    "        for caption_list in caps:\n",
    "            encode_dict_cap = self.tokenizer(text=caption_list, max_length=128, truncation=True, padding='max_length')\n",
    "            input_ids_caps = paddle.to_tensor(encode_dict_cap['input_ids'])\n",
    "            cap_feature, _ = self.ernie(input_ids_caps)\n",
    "            cap_feature = cap_feature.mean(axis=1)  # mean pooling over all captions\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature, axis=0)\n",
    "\n",
    "        # Attention between qcap and caps\n",
    "        caps_feature = self.attention_text(qcap_feature, caps_feature, caps_feature)\n",
    "\n",
    "        # Attention over image features\n",
    "        # imgs_features = paddle.stack(imgs_features, axis=0)  # [B, N, 2048]\n",
    "        qImg_feature = qImg_feature.unsqueeze(1)  # [B, 1, 2048]\n",
    "        imgs_features = self.attention_image(qImg_feature, imgs_features, imgs_features)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        feature = paddle.concat(\n",
    "            [qcap_feature[:, 0, :], caps_feature[:, 0, :], qImg_feature.squeeze(1), imgs_features.squeeze(1)],\n",
    "            axis=-1)\n",
    "        logits = self.classifier1(feature)\n",
    "        logits = self.classifier2(logits)\n",
    "        return logits\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')  # 或 'weighted' 视情况而定\n",
    "    print(f\"Eval loss: {np.mean(losses):.5f}, acc: {acc:.5f}, f1: {f1:.5f}\")\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "\n",
    "# 声明模型\n",
    "model = NetWork(\"image\")\n",
    "#print(model)\n",
    "\n",
    "\n",
    "\n",
    "# AMP scaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "# 基本参数\n",
    "epochs = 10\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "print(f\"Total steps: {num_training_steps}\")\n",
    "\n",
    "# Cosine decay scheduler\n",
    "base_lr = 5e-5\n",
    "lr_scheduler = CosineAnnealingDecay(learning_rate=base_lr, T_max=num_training_steps)\n",
    "\n",
    "# 文件夹准备\n",
    "save_dir = \"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "# Decay params 设置\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 优化器 + gradient clipping\n",
    "optimizer = AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0),  # ⭐️ Gradient clipping ⭐️\n",
    "    apply_decay_param_fun=lambda x: x in decay_params\n",
    ")\n",
    "\n",
    "# Loss 和 Metric\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# 是否用 GPU\n",
    "if paddle.is_compiled_with_cuda():\n",
    "    paddle.set_device('gpu')\n",
    "else:\n",
    "    paddle.set_device('cpu')\n",
    "\n",
    "# 训练过程\n",
    "def do_train(model, criterion, metric, val_dataloader, train_dataloader, optimizer, lr_scheduler,\n",
    "             save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10):\n",
    "\n",
    "    global_step = 0\n",
    "    best_f1 = 0.0\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loader_progress = tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "        for step, batch in enumerate(train_loader_progress, start=1):\n",
    "            labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "\n",
    "            with paddle.amp.auto_cast():  # ⭐️ AMP 混合精度训练 ⭐️\n",
    "                logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                               caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            # Metric\n",
    "            correct = metric.compute(logits, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # AMP backward + optimizer step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            train_loader_progress.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{acc:.4f}\",\n",
    "                \"step\": global_step\n",
    "            })\n",
    "\n",
    "            # Eval\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n",
    "\n",
    "                if eval_f1 > best_f1:\n",
    "                    best_f1 = eval_f1\n",
    "                    best_model_path = os.path.join(best_dir, 'model_bestpro.pdparams')\n",
    "                    paddle.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"[BEST] Step {global_step} | F1: {eval_f1:.4f} (updated)\")\n",
    "\n",
    "# 训练启动\n",
    "do_train(\n",
    "    model, criterion, metric, val_dataloader, train_dataloader,\n",
    "    optimizer, lr_scheduler,\n",
    "    save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10\n",
    ")\n",
    "\n",
    "# 加载最优模型 + 测试\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = os.path.join(\"best_model\", \"model_bestpro.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "    caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "    logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                   caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "    preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1).numpy()\n",
    "    results.extend(preds.tolist())\n",
    "\n",
    "# 保存\n",
    "pd.DataFrame({\"id\": range(len(results)), \"label\": results}).to_csv(\"result.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15c3c05-9e8c-486f-a076-0ff2e710529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 655/655 [01:34<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.63478, Accuracy: 0.77846, F1 Score (macro): 0.75975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 验证函数（带F1）\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "        gc.collect()\n",
    "        paddle.device.cuda.empty_cache()\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(\"Eval Loss: {:.5f}, Accuracy: {:.5f}, F1 Score (macro): {:.5f}\".format(np.mean(losses), acc, f1))\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "# 加载模型参数\n",
    "params_path = os.path.join(\"best_model\", \"model_bestpro.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "else:\n",
    "    print(\"Best model file not found!\")\n",
    "\n",
    "# 损失函数和评估指标\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# 执行评估\n",
    "eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11677d-1d31-43ea-ab2c-2cee0799d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle1",
   "language": "python",
   "name": "paddle1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
