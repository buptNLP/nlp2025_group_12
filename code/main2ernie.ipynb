{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9608cd-5a61-432a-bb9f-cd8e11d4ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle1/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "WARNING: OMP_NUM_THREADS set to 16, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n",
      "/root/miniconda3/envs/paddle1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3572/4164555512.py:23: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n",
      "W0603 15:12:34.969905  3572 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8\n",
      "W0603 15:12:34.970671  3572 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading cached features from cache_features/train_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/val_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/test_features_cached.pkl\n",
      "train_dataset total samples: 11184\n",
      "val_dataset total samples: 1309\n",
      "test_dataset total samples: 1129\n",
      "({'qImg_feature': array([5.0892222e-01, 1.0130705e-01, 4.1554770e-01, ..., 1.5136348e-01,\n",
      "       7.5677846e-05, 2.2541411e-01], dtype=float32), 'qCap': 'çœ‹åˆ°æœ‰äººè¯´ è¿™è€å¤´è¯´äº†å¥è¯ ä¸æ˜¯æˆ‘é€€ä¼‘äº† è¦æ˜¯æ²¡é€€ä¼‘ ä½ æ—©å°±åœ¨ç‰¢é‡Œäº† è¯´æ˜¯æŸåœ°æ”¿æ³•ç³»ç»Ÿçš„å‰é¢†å¯¼ æ­£å±€çº§å¹²éƒ¨é€€ä¼‘çš„ æˆ‘æƒ³é—®è¿™ç§äººæ•¢è¯´å‡ºè¿™ç§è¯ åœ¨èŒé—´åˆ°åº•', 'imgs_features': [array([0.6166205 , 0.32743877, 0.23644671, ..., 0.6098976 , 0.32837993,\n",
      "       0.09397861], dtype=float32), array([0.23688133, 0.73640347, 0.12396187, ..., 0.24109195, 0.10670266,\n",
      "       0.16786775], dtype=float32), array([0.30910894, 0.14095385, 0.31212616, ..., 0.0553519 , 0.00824548,\n",
      "       0.04955674], dtype=float32)], 'caption': ['Boston Orange  æ³¢å£«é “èŠå­: æœ±å­¦æ¸Š  - ç‚ºä¸­åœ‹å²å­¸çš„å¯¦è­‰åŒ–è€ŒåŠªåŠ›', 'æ–°åæ¯æ—¥ç”µè®¯-å¾®æŠ¥çº¸-2021å¹´11æœˆ19æ—¥', 'æ–°åæ¯æ—¥ç”µè®¯-å¾®æŠ¥çº¸-2022å¹´01æœˆ28æ—¥']}, 3, 3)\n",
      "[Tensor(shape=[2], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [2, 2]), [['ç”·å­åœ¨é©¬è·¯ä¸­é—´èººä¸‹ï¼Œå“­ç€æ±‚æ°‘è­¦æ‹˜ç•™ï¼ŒèƒŒååŸå› è®©äººå“­ç¬‘ä¸å¾— - çŸ¥ä¹', 'å¦ˆå¦ˆå‘å‰å†²å†²å†²_ç™¾åº¦ç™¾ç§‘', '', ''], ['å‘¨æ·± Zhou Shen-Latest zhou shen songs-ã€Š50é¦–ä½ æ²’è½éçš„æ­Œã€‹ Best Songs Of Zhou Shenâ©èµ·é£äº†\\\\My Only\\\\è¯·ç¬ƒä¿¡ä¸€ä¸ªæ¢¦\\\\å¤§é±¼\\\\æ‚¬å´–ä¹‹ä¸Š - YouTube', 'ã€å‘¨æ·± Zhou Shenã€‘ã€ç„¡å»£å‘Šã€‘å‘¨æ·±å¥½è½çš„48é¦–æ­Œ,å‘¨æ·± 2022 Best Songs Of Zhou Shenâ©ã€Šæ‚¬å´–ä¹‹ä¸Šã€‹ã€Šæ˜æœˆä¼ è¯´ã€‹ã€Šæ— æ‰€ç•æƒ§ã€‹ã€Šèµ·é£äº†ã€‹ã€Šå½’å¤„ã€‹ - YouTube', 'æ„Ÿè°¢Jackçš„è€å¿ƒã€ä¸“ä¸šå’Œè®¤çœŸï¼Œæ„Ÿè°¢è´µåº—çš„é«˜æ•ˆç‡å’Œå‘¨å…¨æœåŠ¡ã€‚æœ¬æ¬¡è®¢åˆ¶äº† ...', 'on Twitter: \"ğŸ¦ï¸ï¼š1keï¼Œä»€ä¹ˆæ˜¯è„è¯ï¼Ÿ âœ’ï¸ï¼šä½ åœ¨æ¾³å¤§åˆ©äºšçš„å¯’å†¬å’Œæˆ‘ ...']], Tensor(shape=[2, 4, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[[0.02148909, 0.18088770, 0.13741083, ..., 0.77582830,\n",
      "          0.40872678, 0.97752249],\n",
      "         [0.20685357, 0.44522163, 0.50393158, ..., 0.19703768,\n",
      "          0.05795195, 1.85520887],\n",
      "         [0.14578107, 0.42437994, 0.50685716, ..., 0.20471032,\n",
      "          0.07071288, 1.97790229],\n",
      "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "          0.        , 0.        ]],\n",
      "\n",
      "        [[0.15089634, 0.29504406, 0.01835409, ..., 0.54886931,\n",
      "          0.29601479, 0.38991922],\n",
      "         [0.41303921, 0.10840853, 0.23862442, ..., 0.13090843,\n",
      "          0.00549662, 0.38355783],\n",
      "         [0.08522636, 0.00461070, 0.55430305, ..., 0.        ,\n",
      "          0.05064519, 1.06507289],\n",
      "         [0.98064590, 0.14780159, 0.52477348, ..., 0.46662891,\n",
      "          0.16661754, 0.52890837]]]), ['çªç„¶é—´è§‰å¾—è‡ªå·±çœ‹é”™äººäº† æ˜†å±±å¸‚', 'å¥½å¤§æˆ‘è¯´çš„è¡¬è¡£'], Tensor(shape=[2, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[0.43999410, 0.10849334, 0.17837226, ..., 0.00874511, 0.01126958,\n",
      "         0.11103763],\n",
      "        [0.24572070, 0.04100119, 0.00570051, ..., 0.40616715, 0.42247838,\n",
      "         0.78279054]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-03 15:12:38,738] [    INFO]\u001b[0m - Loading weights file from cache at /root/.paddlenlp/models/ernie-m-base/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-06-03 15:12:40,179] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-06-03 15:12:41,664] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieMModel.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-06-03 15:12:41,665] [    INFO]\u001b[0m - All the weights of ErnieMModel were initialized from the model checkpoint at ernie-m-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieMModel for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-06-03 15:12:42,377] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-m-base/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-06-03 15:12:42,379] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/ernie-m-base/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55920 5592\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/5592 [00:00<?, ?it/s]W0603 15:12:42.836725  3572 gpu_resources.cc:306] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\n",
      "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:30<00:00, 12.08it/s, loss=0.1949, acc=0.6440, step=5592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.50273, acc: 0.79985, f1: 0.75351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:36<00:00, 12.24it/s, loss=0.1949, acc=0.6440, step=5592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 5592 | F1: 0.7535 (updated)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:15<00:00, 14.05it/s, loss=0.1171, acc=0.8259, step=11184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.34302, acc: 0.87930, f1: 0.85989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:34<00:00, 12.30it/s, loss=0.1171, acc=0.8259, step=11184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 11184 | F1: 0.8599 (updated)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5590/5592 [07:28<00:00, 10.77it/s, loss=0.1041, acc=0.8780, step=16776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.27895, acc: 0.90069, f1: 0.88582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00,  3.07s/it, loss=0.1041, acc=0.8780, step=16776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 16776 | F1: 0.8858 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00, 12.19it/s, loss=0.1041, acc=0.8780, step=16776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:30<00:00, 13.07it/s, loss=0.0941, acc=0.8967, step=22368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.25271, acc: 0.91291, f1: 0.90179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:37<00:00,  3.56s/it, loss=0.0941, acc=0.8967, step=22368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 22368 | F1: 0.9018 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00, 12.20it/s, loss=0.0941, acc=0.8967, step=22368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:32<00:00, 12.30it/s, loss=0.0118, acc=0.9151, step=27960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.22753, acc: 0.91597, f1: 0.90503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:37<00:00,  3.73s/it, loss=0.0118, acc=0.9151, step=27960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 27960 | F1: 0.9050 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00, 12.21it/s, loss=0.0118, acc=0.9151, step=27960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5590/5592 [07:34<00:00, 12.90it/s, loss=0.4126, acc=0.9298, step=33552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.23047, acc: 0.91902, f1: 0.90880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00,  3.03s/it, loss=0.4126, acc=0.9298, step=33552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 33552 | F1: 0.9088 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00, 12.19it/s, loss=0.4126, acc=0.9298, step=33552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:17<00:00, 13.61it/s, loss=0.0101, acc=0.9376, step=39144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.22951, acc: 0.91902, f1: 0.90933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:37<00:00,  3.55s/it, loss=0.0101, acc=0.9376, step=39144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 39144 | F1: 0.9093 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:37<00:00, 12.21it/s, loss=0.0101, acc=0.9376, step=39144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:20<00:00, 13.92it/s, loss=0.0136, acc=0.9445, step=44736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.22480, acc: 0.92437, f1: 0.91381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:39<00:00,  3.45s/it, loss=0.0136, acc=0.9445, step=44736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 44736 | F1: 0.9138 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:40<00:00, 12.16it/s, loss=0.0136, acc=0.9445, step=44736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5591/5592 [07:28<00:00, 12.76it/s, loss=0.0515, acc=0.9481, step=50328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.21813, acc: 0.92513, f1: 0.91495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:38<00:00,  3.71s/it, loss=0.0515, acc=0.9481, step=50328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 50328 | F1: 0.9149 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:39<00:00, 12.16it/s, loss=0.0515, acc=0.9481, step=50328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:39<00:00,  3.03s/it, loss=0.0181, acc=0.9542, step=55920]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.21909, acc: 0.92437, f1: 0.91468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5592/5592 [07:39<00:00, 12.16it/s, loss=0.0181, acc=0.9542, step=55920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1129/1129 [00:22<00:00, 49.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os\n",
    "import imghdr\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# æ›´é€šç”¨çš„å†™æ³•ï¼Œå…¼å®¹ Jupyter å’Œè„šæœ¬è¿è¡Œ\n",
    "try:\n",
    "    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'autodl-tmp/queries_dataset_merge')\n",
    "\n",
    "paddle.set_device('gpu')  # å¦‚æœæ²¡ GPUï¼Œå¯ä»¥æ”¹ä¸º 'cpu'\n",
    "\n",
    "#è¯»å–æ•°æ®\n",
    "import json\n",
    "data_items_train = json.load(open(os.path.join(dataset_dir, 'dataset_items_train.json'), encoding='utf-8'))\n",
    "data_items_val = json.load(open(os.path.join(dataset_dir, 'dataset_items_val.json'), encoding='utf-8'))\n",
    "data_items_test = json.load(open(os.path.join(dataset_dir, 'dataset_items_test.json'), encoding='utf-8'))\n",
    "\n",
    "\n",
    "#è¯»å–æ•°æ®ä¸­çš„æ¯ä¸€ä¸ªæ ·æœ¬ï¼šå›¾åƒimgã€æ–‡æœ¬captionã€\n",
    "#å¯¹åº”çš„img_html_newsã€inverse_searchä¸ºæ”¯æŒå›¾åƒimgå’Œæ–‡æœ¬captionçš„è¯æ®ææ–™\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>', '')\n",
    "    input_str = input_str.replace('</b>', '')\n",
    "    # input_str = unidecode(input_str)\n",
    "    return input_str\n",
    "\n",
    "\n",
    "class FeatureCachedNewsContextDataset(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split, resnet_model, cache_dir='cache_features'):\n",
    "        self.cache_path = os.path.join(cache_dir, f'{split}_features_cached.pkl')\n",
    "        self.split = split\n",
    "        self.resnet = resnet_model\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(self.cache_path):\n",
    "            print(f\"[INFO] Loading cached features from {self.cache_path}\")\n",
    "            with open(self.cache_path, 'rb') as f:\n",
    "                self.samples = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"[INFO] Creating cache with CNN features for {split} set...\")\n",
    "            self.samples = self.preprocess_and_cache(context_data_items_dict, queries_root_dir)\n",
    "            with open(self.cache_path, 'wb') as f:\n",
    "                pickle.dump(self.samples, f)\n",
    "            print(f\"[INFO] Cached features saved to {self.cache_path}\")\n",
    "\n",
    "    def preprocess_and_cache(self, data_dict, queries_root_dir):\n",
    "        from PIL import Image\n",
    "        import imghdr\n",
    "        from paddle.vision import transforms as T\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        def load_image(image_path):\n",
    "            try:\n",
    "                if imghdr.what(image_path) == 'gif':\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                else:\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                return transform(img)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        def process_string(text):\n",
    "            return text.replace('&#39;', ' ').replace('<b>', '').replace('</b>', '')\n",
    "\n",
    "        def extract_captions(inv_dict, direct_dict):\n",
    "            captions = []\n",
    "            for key in ['all_fully_matched_captions', 'all_partially_matched_captions']:\n",
    "                for page in inv_dict.get(key, []):\n",
    "                    if 'title' in page:\n",
    "                        captions.append(process_string(page['title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            for key in ['images_with_captions', 'images_with_caption_matched_tags', 'images_with_no_captions']:\n",
    "                for page in direct_dict.get(key, []):\n",
    "                    if 'page_title' in page:\n",
    "                        captions.append(process_string(page['page_title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            return list(set(captions))\n",
    "\n",
    "        MAX_IMG_PER_SAMPLE = 100\n",
    "        samples = []\n",
    "\n",
    "        for key in tqdm(data_dict, desc=f\"Processing {self.split} with features\"):\n",
    "            item = data_dict[key]\n",
    "            try:\n",
    "                qimg_path = os.path.join(queries_root_dir, item['image_path'])\n",
    "                qimg_tensor = load_image(qimg_path)\n",
    "                if qimg_tensor is None:\n",
    "                    continue\n",
    "                qImg_feature = self.resnet(qimg_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "                direct_path = os.path.join(queries_root_dir, item['direct_path'])\n",
    "                inverse_path = os.path.join(queries_root_dir, item['inv_path'])\n",
    "\n",
    "                with open(os.path.join(direct_path, 'direct_annotation.json'), encoding='utf-8') as f:\n",
    "                    direct_dict = json.load(f)\n",
    "                with open(os.path.join(inverse_path, 'inverse_annotation.json'), encoding='utf-8') as f:\n",
    "                    inv_dict = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {e}, skipping {key}\")\n",
    "                continue\n",
    "\n",
    "            evidence_features = []\n",
    "            for key1 in ['images_with_captions', 'images_with_no_captions', 'images_with_caption_matched_tags']:\n",
    "                pages = direct_dict.get(key1, [])\n",
    "                for i, page in enumerate(pages):\n",
    "                    if i >= MAX_IMG_PER_SAMPLE:\n",
    "                        break\n",
    "                    img_path = os.path.join(direct_path, page['image_path'].split('/')[-1])\n",
    "                    img_tensor = load_image(img_path)\n",
    "                    if img_tensor is not None:\n",
    "                        img_feature = self.resnet(img_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "                        evidence_features.append(img_feature)\n",
    "\n",
    "            if len(evidence_features) == 0:\n",
    "                continue\n",
    "\n",
    "            captions = extract_captions(inv_dict, direct_dict)\n",
    "            sample = {\n",
    "                'qImg_feature': qImg_feature,\n",
    "                'qCap': item['caption'],\n",
    "                'imgs_features': evidence_features,\n",
    "                'caption': captions\n",
    "            }\n",
    "            if self.split != 'test':\n",
    "                sample['label'] = int(item['label'])\n",
    "\n",
    "            samples.append(sample)\n",
    "\n",
    "            # æ˜¾å­˜æ¸…ç†\n",
    "            del qImg_feature, evidence_features, img_tensor, qimg_tensor\n",
    "            gc.collect()\n",
    "            paddle.device.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[INFO] Cached {len(samples)} samples for {self.split}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.split != 'test':\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "        else:\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "\n",
    "from paddle.vision import models\n",
    "from paddle import nn\n",
    "import paddle\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch='resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        elif resnet_arch == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet arch: {resnet_arch}\")\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0], out.shape[1]))\n",
    "        return out\n",
    "\n",
    "#### load Datasets ####\n",
    "resnet = EncoderCNN(resnet_arch='resnet50')\n",
    "resnet.eval()\n",
    "train_dataset = FeatureCachedNewsContextDataset(data_items_train, dataset_dir, 'train', resnet)\n",
    "val_dataset = FeatureCachedNewsContextDataset(data_items_val, dataset_dir, 'val', resnet)\n",
    "test_dataset = FeatureCachedNewsContextDataset(data_items_test, dataset_dir, 'test', resnet)\n",
    "print(f\"train_dataset total samples: {len(train_dataset)}\")\n",
    "print(f\"val_dataset total samples: {len(val_dataset)}\")\n",
    "print(f\"test_dataset total samples: {len(test_dataset)}\")\n",
    "\n",
    "# æ‰“å°æ•°æ®\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "def collate_context_cached_train(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch, labels = [], [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "        labels.append(paddle.to_tensor(sample['label']))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "    labels = paddle.stack(labels)\n",
    "\n",
    "    return labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "def collate_context_cached_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch = [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "\n",
    "    return caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "\n",
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "                              collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False,\n",
    "                            collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                             collate_fn=collate_context_cached_test, return_list=True, num_workers=2)\n",
    "\n",
    "# æ‰“å°æ•°æ®\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "#æ¨¡å‹æ„å»º\n",
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.attention_text = nn.MultiHeadAttention(embed_dim=768, num_heads=16)\n",
    "        self.attention_image = nn.MultiHeadAttention(embed_dim=2048, num_heads=16)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768, 3)\n",
    "        else:\n",
    "            self.classifier1 = nn.Linear(2 * (768 + 2048), 1024)\n",
    "            self.classifier2 = nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, qCap, qImg_feature, caps, imgs_features):\n",
    "        # Encode qCap\n",
    "        encode_dict_qcap = self.tokenizer(text=qCap, max_length=128, truncation=True, padding='max_length')\n",
    "        input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "        qcap_feature, _ = self.ernie(input_ids_qcap)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:, 0, :])\n",
    "            return logits\n",
    "\n",
    "        # Encode all evidence captions\n",
    "        caps_feature = []\n",
    "        for caption_list in caps:\n",
    "            encode_dict_cap = self.tokenizer(text=caption_list, max_length=128, truncation=True, padding='max_length')\n",
    "            input_ids_caps = paddle.to_tensor(encode_dict_cap['input_ids'])\n",
    "            cap_feature, _ = self.ernie(input_ids_caps)\n",
    "            cap_feature = cap_feature.mean(axis=1)  # mean pooling over all captions\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature, axis=0)\n",
    "\n",
    "        # Attention between qcap and caps\n",
    "        caps_feature = self.attention_text(qcap_feature, caps_feature, caps_feature)\n",
    "\n",
    "        # Attention over image features\n",
    "        # imgs_features = paddle.stack(imgs_features, axis=0)  # [B, N, 2048]\n",
    "        qImg_feature = qImg_feature.unsqueeze(1)  # [B, 1, 2048]\n",
    "        imgs_features = self.attention_image(qImg_feature, imgs_features, imgs_features)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        feature = paddle.concat(\n",
    "            [qcap_feature[:, 0, :], caps_feature[:, 0, :], qImg_feature.squeeze(1), imgs_features.squeeze(1)],\n",
    "            axis=-1)\n",
    "        logits = self.classifier1(feature)\n",
    "        logits = self.classifier2(logits)\n",
    "        return logits\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')  # æˆ– 'weighted' è§†æƒ…å†µè€Œå®š\n",
    "    print(f\"Eval loss: {np.mean(losses):.5f}, acc: {acc:.5f}, f1: {f1:.5f}\")\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "\n",
    "# å£°æ˜æ¨¡å‹\n",
    "model = NetWork(\"image\")\n",
    "#print(model)\n",
    "\n",
    "epochs = 10\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "print(num_training_steps,warmup_steps)\n",
    "\n",
    "# å®šä¹‰ learning_rate_schedulerï¼Œè´Ÿè´£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ lr è¿›è¡Œè°ƒåº¦\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-6, num_training_steps, warmup_steps)\n",
    "# è®­ç»ƒç»“æŸåï¼Œå­˜å‚¨æ¨¡å‹å‚æ•°\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# åˆ›å»ºä¿å­˜çš„æ–‡ä»¶å¤¹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# å®šä¹‰ Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# äº¤å‰ç†µæŸå¤±\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# è¯„ä¼°çš„æ—¶å€™é‡‡ç”¨å‡†ç¡®ç‡æŒ‡æ ‡\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "if paddle.is_compiled_with_cuda():\n",
    "    paddle.set_device('gpu')\n",
    "else:\n",
    "    paddle.set_device('cpu')\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒ\n",
    "def do_train(model, criterion, metric, val_dataloader, train_dataloader, optimizer, lr_scheduler,\n",
    "             save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10):\n",
    "\n",
    "    global_step = 0\n",
    "    best_f1 = 0.0\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loader_progress = tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "        for step, batch in enumerate(train_loader_progress, start=1):\n",
    "            labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "\n",
    "            logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                           caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            correct = metric.compute(logits, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            train_loader_progress.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{acc:.4f}\",\n",
    "                \"step\": global_step\n",
    "            })\n",
    "\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n",
    "\n",
    "                if eval_f1 > best_f1:\n",
    "                    best_f1 = eval_f1\n",
    "                    best_model_path = os.path.join(best_dir, 'model_bestinitial.pdparams')\n",
    "                    paddle.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"[BEST] Step {global_step} | F1: {eval_f1:.4f} (updated)\")\n",
    "\n",
    "\n",
    "do_train(\n",
    "    model, criterion, metric, val_dataloader, train_dataloader,\n",
    "    optimizer, lr_scheduler,\n",
    "    save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10\n",
    ")\n",
    "\n",
    "\n",
    "# æ ¹æ®å®é™…è¿è¡Œæƒ…å†µï¼Œæ›´æ¢åŠ è½½çš„å‚æ•°è·¯å¾„\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "# åŠ è½½æœ€ä¼˜æ¨¡å‹\n",
    "params_path = os.path.join(\"best_model\", \"model_bestinitial.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "    caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "    logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                   caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "    preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1).numpy()\n",
    "    results.extend(preds.tolist())\n",
    "\n",
    "# ä¿å­˜\n",
    "pd.DataFrame({\"id\": range(len(results)), \"label\": results}).to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15c3c05-9e8c-486f-a076-0ff2e710529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 655/655 [01:36<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.21813, Accuracy: 0.92513, F1 Score (macro): 0.91495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# éªŒè¯å‡½æ•°ï¼ˆå¸¦F1ï¼‰\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "        gc.collect()\n",
    "        paddle.device.cuda.empty_cache()\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(\"Eval Loss: {:.5f}, Accuracy: {:.5f}, F1 Score (macro): {:.5f}\".format(np.mean(losses), acc, f1))\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å‚æ•°\n",
    "params_path = os.path.join(\"best_model\", \"model_bestinitial.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "else:\n",
    "    print(\"Best model file not found!\")\n",
    "\n",
    "# æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11677d-1d31-43ea-ab2c-2cee0799d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle1",
   "language": "python",
   "name": "paddle1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
