{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9608cd-5a61-432a-bb9f-cd8e11d4ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle1/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "WARNING: OMP_NUM_THREADS set to 16, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n",
      "/root/miniconda3/envs/paddle1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1525/2479717948.py:23: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n",
      "W0613 03:48:46.348315  1525 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8\n",
      "W0613 03:48:46.349671  1525 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading cached features from cache_features/train_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/val_features_cached.pkl\n",
      "[INFO] Loading cached features from cache_features/test_features_cached.pkl\n",
      "train_dataset total samples: 11184\n",
      "val_dataset total samples: 1309\n",
      "test_dataset total samples: 1129\n",
      "({'qImg_feature': array([5.0892222e-01, 1.0130705e-01, 4.1554770e-01, ..., 1.5136348e-01,\n",
      "       7.5677846e-05, 2.2541411e-01], dtype=float32), 'qCap': '看到有人说 这老头说了句话 不是我退休了 要是没退休 你早就在牢里了 说是某地政法系统的前领导 正局级干部退休的 我想问这种人敢说出这种话 在职间到底', 'imgs_features': [array([0.6166205 , 0.32743877, 0.23644671, ..., 0.6098976 , 0.32837993,\n",
      "       0.09397861], dtype=float32), array([0.23688133, 0.73640347, 0.12396187, ..., 0.24109195, 0.10670266,\n",
      "       0.16786775], dtype=float32), array([0.30910894, 0.14095385, 0.31212616, ..., 0.0553519 , 0.00824548,\n",
      "       0.04955674], dtype=float32)], 'caption': ['Boston Orange  波士頓菊子: 朱学渊  - 為中國史學的實證化而努力', '新华每日电讯-微报纸-2021年11月19日', '新华每日电讯-微报纸-2022年01月28日']}, 3, 3)\n",
      "[Tensor(shape=[2], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [0, 2]), [['芬兰于4月4日加入北约 芬兰总统将前往布鲁塞尔出席入约仪式 女总理落选', '芬兰于4月4日加入北约 芬兰总统将前往布鲁塞尔出席入约仪式 女总理落选_军事频道_中华网', '普京反击快准狠！“芬兰加入北约”突响，俄官宣重大决定，30国瑟瑟发抖', '芬兰正式加入北约 俄罗斯表示北欧地区局势由此发生根本变化_新闻频道_央视网(cctv.com)', '芬兰将于4月4日加入北约，俄方回应|芬兰|北约|俄罗斯_新浪新闻'], ['Jonathan Wolan (@MrWolan) / টুইটার', 'Kristen Finn (@KEF337) / Twitter', '', '', '']], Tensor(shape=[2, 5, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[[0.29030082, 0.31047672, 0.11954876, ..., 0.55047816,\n",
      "          0.00848733, 0.39180395],\n",
      "         [0.18811564, 0.43830323, 0.12051324, ..., 2.27471423,\n",
      "          0.17078483, 0.31116858],\n",
      "         [0.41668147, 0.51315188, 0.04565437, ..., 0.20077953,\n",
      "          0.07497751, 0.20067939],\n",
      "         [0.06245151, 0.63518471, 0.28263143, ..., 0.28409332,\n",
      "          0.03130679, 0.47538337],\n",
      "         [0.45002317, 0.88215911, 0.04152051, ..., 0.34022391,\n",
      "          0.09042261, 0.08405863]],\n",
      "\n",
      "        [[0.16757704, 0.41723001, 0.31104422, ..., 0.73577559,\n",
      "          0.03731612, 0.55346775],\n",
      "         [0.32327190, 0.04735380, 0.40516010, ..., 0.10328159,\n",
      "          0.03644428, 0.51313484],\n",
      "         [0.26092064, 0.04818355, 0.47371462, ..., 0.05750643,\n",
      "          0.00634462, 0.44312400],\n",
      "         [0.14647695, 0.01274437, 0.30086693, ..., 0.41478586,\n",
      "          0.24748731, 0.38276511],\n",
      "         [0.48211491, 0.12413098, 0.94015527, ..., 0.09438735,\n",
      "          0.17679000, 0.42297286]]]), ['芬兰总统办公室3日确认芬兰将于4月4日正式成为北大西洋公约组织成员国', 'few snaps from altrinchamfc v DagRedFC in todays TheVanaramaNL '], Tensor(shape=[2, 2048], dtype=float32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[0.26440510, 0.27666172, 0.70167130, ..., 0.34221119, 0.05966476,\n",
      "         0.37354359],\n",
      "        [0.35736051, 0.23416750, 0.01339060, ..., 0.40486491, 0.41165721,\n",
      "         0.12868442]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)ers/ernie_3.0/ernie_3.0_base_zh.pdparams: 100%|██████████| 474M/474M [00:39<00:00, 12.1MB/s] \n",
      "\u001b[32m[2025-06-13 03:49:30,586] [    INFO]\u001b[0m - Loading weights file from cache at /root/.paddlenlp/models/ernie-3.0-base-zh/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:49:31,057] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[33m[2025-06-13 03:49:31,682] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-3.0-base-zh were not used when initializing ErnieModel: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:49:31,684] [    INFO]\u001b[0m - All the weights of ErnieModel were initialized from the model checkpoint at ernie-3.0-base-zh.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieModel for predictions without further training.\u001b[0m\n",
      "(…)rs/ernie_3.0/ernie_3.0_base_zh_vocab.txt: 100%|██████████| 187k/187k [00:00<00:00, 17.8MB/s]\n",
      "\u001b[32m[2025-06-13 03:49:31,771] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-06-13 03:49:31,772] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 55920\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/5592 [00:00<?, ?it/s]W0613 03:49:32.448763  1525 gpu_resources.cc:306] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\n",
      "Training Epoch 1:   6%|▋         | 359/5592 [00:35<07:49, 11.15it/s, loss=6.8661, acc=0.6100, step=359] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 1024.0, decrease to: 1024.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|█████████▉| 5591/5592 [08:59<00:00, 11.40it/s, loss=0.1545, acc=0.6824, step=5592] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.25958, acc: 0.69901, f1: 0.63306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5592/5592 [09:01<00:00, 10.33it/s, loss=0.1545, acc=0.6824, step=5592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 5592 | F1: 0.6331 (updated)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|█████████▉| 5590/5592 [08:48<00:00, 11.44it/s, loss=0.0003, acc=0.7390, step=11184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.89711, acc: 0.75248, f1: 0.72887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5592/5592 [08:57<00:00, 10.41it/s, loss=0.0003, acc=0.7390, step=11184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 11184 | F1: 0.7289 (updated)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  22%|██▏       | 1240/5592 [01:56<06:29, 11.18it/s, loss=2.3641, acc=0.7901, step=12425]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  58%|█████▊    | 3256/5592 [05:03<03:30, 11.11it/s, loss=0.4195, acc=0.7849, step=14440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  94%|█████████▍| 5257/5592 [08:10<00:30, 11.15it/s, loss=0.0523, acc=0.7820, step=16441] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5592/5592 [08:58<00:00,  2.72s/it, loss=2.6002, acc=0.7808, step=16776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.91749, acc: 0.75248, f1: 0.72737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5592/5592 [08:58<00:00, 10.38it/s, loss=2.6002, acc=0.7808, step=16776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  31%|███▏      | 1760/5592 [02:42<06:03, 10.54it/s, loss=0.0004, acc=0.8165, step=18536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  57%|█████▋    | 3211/5592 [04:57<03:45, 10.54it/s, loss=1.8035, acc=0.8107, step=2e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|█████████▉| 5589/5592 [08:38<00:00, 10.72it/s, loss=1.9540, acc=0.8138, step=22365] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5592/5592 [08:56<00:00,  3.55s/it, loss=2.0364, acc=0.8138, step=22368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.40730, acc: 0.72040, f1: 0.65938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5592/5592 [08:56<00:00, 10.42it/s, loss=2.0364, acc=0.8138, step=22368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  43%|████▎     | 2426/5592 [03:45<04:53, 10.78it/s, loss=0.0000, acc=0.8482, step=24795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  80%|███████▉  | 4466/5592 [06:55<01:37, 11.60it/s, loss=2.3904, acc=0.8479, step=26835] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|█████████▉| 5590/5592 [08:55<00:00, 11.64it/s, loss=0.0000, acc=0.8486, step=27960] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.14575, acc: 0.76929, f1: 0.75322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5592/5592 [09:00<00:00,  2.98s/it, loss=0.0000, acc=0.8486, step=27960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] Step 27960 | F1: 0.7532 (updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5592/5592 [09:00<00:00, 10.35it/s, loss=0.0000, acc=0.8486, step=27960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  18%|█▊        | 1008/5592 [01:33<07:51,  9.72it/s, loss=0.0004, acc=0.8686, step=28968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  54%|█████▍    | 3047/5592 [04:42<03:57, 10.72it/s, loss=0.1296, acc=0.8748, step=31007] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  90%|█████████ | 5050/5592 [07:49<00:46, 11.59it/s, loss=0.0001, acc=0.8784, step=33010] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5592/5592 [08:56<00:00,  2.68s/it, loss=0.0517, acc=0.8789, step=33552] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.37997, acc: 0.75477, f1: 0.72397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5592/5592 [08:57<00:00, 10.40it/s, loss=0.0517, acc=0.8789, step=33552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  27%|██▋       | 1483/5592 [02:18<06:06, 11.22it/s, loss=1.7269, acc=0.9103, step=35035]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  62%|██████▏   | 3489/5592 [05:25<03:07, 11.23it/s, loss=0.0130, acc=0.9111, step=37041] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:  91%|█████████ | 5063/5592 [07:52<00:45, 11.65it/s, loss=0.0000, acc=0.9080, step=38615] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5592/5592 [08:57<00:00,  3.53s/it, loss=0.0000, acc=0.9078, step=39144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.53150, acc: 0.75095, f1: 0.73543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5592/5592 [08:58<00:00, 10.39it/s, loss=0.0000, acc=0.9078, step=39144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  42%|████▏     | 2359/5592 [03:40<05:25,  9.94it/s, loss=0.0007, acc=0.9328, step=41503] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  82%|████████▏ | 4591/5592 [07:07<01:24, 11.91it/s, loss=0.0000, acc=0.9356, step=43736] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5592/5592 [08:57<00:00,  3.21s/it, loss=0.0003, acc=0.9359, step=44736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.68262, acc: 0.76471, f1: 0.74671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5592/5592 [08:57<00:00, 10.39it/s, loss=0.0003, acc=0.9359, step=44736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  20%|█▉        | 1117/5592 [01:44<06:25, 11.62it/s, loss=0.0002, acc=0.9512, step=45853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  61%|██████    | 3391/5592 [05:16<03:22, 10.87it/s, loss=0.0000, acc=0.9500, step=48128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  97%|█████████▋| 5398/5592 [08:20<00:17, 10.94it/s, loss=0.0001, acc=0.9511, step=50134] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5592/5592 [08:57<00:00,  3.60s/it, loss=0.2084, acc=0.9514, step=50328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.75636, acc: 0.76776, f1: 0.74875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5592/5592 [08:58<00:00, 10.39it/s, loss=0.2084, acc=0.9514, step=50328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  32%|███▏      | 1811/5592 [02:48<05:29, 11.49it/s, loss=3.3296, acc=0.9602, step=52139] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  69%|██████▉   | 3874/5592 [05:59<02:55,  9.79it/s, loss=0.0000, acc=0.9604, step=54203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5592/5592 [08:58<00:00,  5.40s/it, loss=0.0014, acc=0.9616, step=55920] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.77541, acc: 0.76776, f1: 0.74729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5592/5592 [08:59<00:00, 10.37it/s, loss=0.0014, acc=0.9616, step=55920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1129/1129 [00:26<00:00, 42.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os\n",
    "import imghdr\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.optimizer import AdamW\n",
    "from paddle import nn\n",
    "\n",
    "\n",
    "# 更通用的写法，兼容 Jupyter 和脚本运行\n",
    "try:\n",
    "    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'autodl-tmp/queries_dataset_merge')\n",
    "\n",
    "paddle.set_device('gpu')  # 如果没 GPU，可以改为 'cpu'\n",
    "\n",
    "#读取数据\n",
    "import json\n",
    "data_items_train = json.load(open(os.path.join(dataset_dir, 'dataset_items_train.json'), encoding='utf-8'))\n",
    "data_items_val = json.load(open(os.path.join(dataset_dir, 'dataset_items_val.json'), encoding='utf-8'))\n",
    "data_items_test = json.load(open(os.path.join(dataset_dir, 'dataset_items_test.json'), encoding='utf-8'))\n",
    "\n",
    "\n",
    "#读取数据中的每一个样本：图像img、文本caption、\n",
    "#对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>', '')\n",
    "    input_str = input_str.replace('</b>', '')\n",
    "    # input_str = unidecode(input_str)\n",
    "    return input_str\n",
    "\n",
    "\n",
    "class FeatureCachedNewsContextDataset(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split, resnet_model, cache_dir='cache_features'):\n",
    "        self.cache_path = os.path.join(cache_dir, f'{split}_features_cached.pkl')\n",
    "        self.split = split\n",
    "        self.resnet = resnet_model\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(self.cache_path):\n",
    "            print(f\"[INFO] Loading cached features from {self.cache_path}\")\n",
    "            with open(self.cache_path, 'rb') as f:\n",
    "                self.samples = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"[INFO] Creating cache with CNN features for {split} set...\")\n",
    "            self.samples = self.preprocess_and_cache(context_data_items_dict, queries_root_dir)\n",
    "            with open(self.cache_path, 'wb') as f:\n",
    "                pickle.dump(self.samples, f)\n",
    "            print(f\"[INFO] Cached features saved to {self.cache_path}\")\n",
    "\n",
    "    def preprocess_and_cache(self, data_dict, queries_root_dir):\n",
    "        from PIL import Image\n",
    "        import imghdr\n",
    "        from paddle.vision import transforms as T\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        def load_image(image_path):\n",
    "            try:\n",
    "                if imghdr.what(image_path) == 'gif':\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                else:\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        img = Image.open(f).convert('RGB')\n",
    "                return transform(img)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        def process_string(text):\n",
    "            return text.replace('&#39;', ' ').replace('<b>', '').replace('</b>', '')\n",
    "\n",
    "        def extract_captions(inv_dict, direct_dict):\n",
    "            captions = []\n",
    "            for key in ['all_fully_matched_captions', 'all_partially_matched_captions']:\n",
    "                for page in inv_dict.get(key, []):\n",
    "                    if 'title' in page:\n",
    "                        captions.append(process_string(page['title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            for key in ['images_with_captions', 'images_with_caption_matched_tags', 'images_with_no_captions']:\n",
    "                for page in direct_dict.get(key, []):\n",
    "                    if 'page_title' in page:\n",
    "                        captions.append(process_string(page['page_title']))\n",
    "                    if 'caption' in page:\n",
    "                        for val in page['caption'].values():\n",
    "                            captions.append(process_string(val))\n",
    "            return list(set(captions))\n",
    "\n",
    "        MAX_IMG_PER_SAMPLE = 100\n",
    "        samples = []\n",
    "\n",
    "        for key in tqdm(data_dict, desc=f\"Processing {self.split} with features\"):\n",
    "            item = data_dict[key]\n",
    "            try:\n",
    "                qimg_path = os.path.join(queries_root_dir, item['image_path'])\n",
    "                qimg_tensor = load_image(qimg_path)\n",
    "                if qimg_tensor is None:\n",
    "                    continue\n",
    "                qImg_feature = self.resnet(qimg_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "                direct_path = os.path.join(queries_root_dir, item['direct_path'])\n",
    "                inverse_path = os.path.join(queries_root_dir, item['inv_path'])\n",
    "\n",
    "                with open(os.path.join(direct_path, 'direct_annotation.json'), encoding='utf-8') as f:\n",
    "                    direct_dict = json.load(f)\n",
    "                with open(os.path.join(inverse_path, 'inverse_annotation.json'), encoding='utf-8') as f:\n",
    "                    inv_dict = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {e}, skipping {key}\")\n",
    "                continue\n",
    "\n",
    "            evidence_features = []\n",
    "            for key1 in ['images_with_captions', 'images_with_no_captions', 'images_with_caption_matched_tags']:\n",
    "                pages = direct_dict.get(key1, [])\n",
    "                for i, page in enumerate(pages):\n",
    "                    if i >= MAX_IMG_PER_SAMPLE:\n",
    "                        break\n",
    "                    img_path = os.path.join(direct_path, page['image_path'].split('/')[-1])\n",
    "                    img_tensor = load_image(img_path)\n",
    "                    if img_tensor is not None:\n",
    "                        img_feature = self.resnet(img_tensor.unsqueeze(0)).detach().cpu().squeeze(0).numpy()\n",
    "                        evidence_features.append(img_feature)\n",
    "\n",
    "            if len(evidence_features) == 0:\n",
    "                continue\n",
    "\n",
    "            captions = extract_captions(inv_dict, direct_dict)\n",
    "            sample = {\n",
    "                'qImg_feature': qImg_feature,\n",
    "                'qCap': item['caption'],\n",
    "                'imgs_features': evidence_features,\n",
    "                'caption': captions\n",
    "            }\n",
    "            if self.split != 'test':\n",
    "                sample['label'] = int(item['label'])\n",
    "\n",
    "            samples.append(sample)\n",
    "\n",
    "            # 显存清理\n",
    "            del qImg_feature, evidence_features, img_tensor, qimg_tensor\n",
    "            gc.collect()\n",
    "            paddle.device.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[INFO] Cached {len(samples)} samples for {self.split}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.split != 'test':\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "        else:\n",
    "            return sample, len(sample['caption']), len(sample['imgs_features'])\n",
    "\n",
    "from paddle.vision import models\n",
    "from paddle import nn\n",
    "import paddle\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch='resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        elif resnet_arch == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet arch: {resnet_arch}\")\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0], out.shape[1]))\n",
    "        return out\n",
    "\n",
    "#### load Datasets ####\n",
    "resnet = EncoderCNN(resnet_arch='resnet50')\n",
    "resnet.eval()\n",
    "train_dataset = FeatureCachedNewsContextDataset(data_items_train, dataset_dir, 'train', resnet)\n",
    "val_dataset = FeatureCachedNewsContextDataset(data_items_val, dataset_dir, 'val', resnet)\n",
    "test_dataset = FeatureCachedNewsContextDataset(data_items_test, dataset_dir, 'test', resnet)\n",
    "print(f\"train_dataset total samples: {len(train_dataset)}\")\n",
    "print(f\"val_dataset total samples: {len(val_dataset)}\")\n",
    "print(f\"test_dataset total samples: {len(test_dataset)}\")\n",
    "\n",
    "# 打印数据\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "def collate_context_cached_train(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch, labels = [], [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "        labels.append(paddle.to_tensor(sample['label']))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "    labels = paddle.stack(labels)\n",
    "\n",
    "    return labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "def collate_context_cached_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_caps = max([item[1] for item in batch])\n",
    "    max_imgs = max([item[2] for item in batch])\n",
    "\n",
    "    qCap_batch, qImg_feature_batch, caps_batch, imgs_feature_batch = [], [], [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        caps = sample['caption'] + [\"\"] * (max_caps - len(sample['caption']))\n",
    "        caps_batch.append(caps)\n",
    "\n",
    "        imgs = sample['imgs_features']\n",
    "        imgs = [paddle.to_tensor(img, dtype='float32') for img in imgs]\n",
    "        pad = [paddle.zeros_like(imgs[0]) for _ in range(max_imgs - len(imgs))]\n",
    "        imgs_padded = imgs + pad\n",
    "        imgs_feature_batch.append(paddle.stack(imgs_padded))\n",
    "\n",
    "        qCap_batch.append(sample['qCap'])\n",
    "        qImg_feature_batch.append(paddle.to_tensor(sample['qImg_feature'], dtype='float32'))\n",
    "\n",
    "    qImg_feature_batch = paddle.stack(qImg_feature_batch, axis=0)\n",
    "    imgs_feature_batch = paddle.stack(imgs_feature_batch, axis=0)\n",
    "\n",
    "    return caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch\n",
    "\n",
    "\n",
    "\n",
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "                              collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False,\n",
    "                            collate_fn=collate_context_cached_train, return_list=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                             collate_fn=collate_context_cached_test, return_list=True, num_workers=2)\n",
    "\n",
    "# 打印数据\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "#模型构建\n",
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieModel, ErnieTokenizer\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.ernie = ErnieModel.from_pretrained('ernie-3.0-base-zh')\n",
    "        self.tokenizer = ErnieTokenizer.from_pretrained('ernie-3.0-base-zh')\n",
    "        self.attention_text = nn.MultiHeadAttention(embed_dim=768, num_heads=16)\n",
    "        self.attention_image = nn.MultiHeadAttention(embed_dim=2048, num_heads=16)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768, 3)\n",
    "        else:\n",
    "            self.classifier1 = nn.Linear(2 * (768 + 2048), 1024)\n",
    "            self.classifier2 = nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, qCap, qImg_feature, caps, imgs_features):\n",
    "        # Encode qCap\n",
    "        encode_dict_qcap = self.tokenizer(text=qCap, max_length=128, truncation=True, padding='max_length')\n",
    "        input_ids_qcap = paddle.to_tensor(encode_dict_qcap['input_ids'])\n",
    "        qcap_feature, _ = self.ernie(input_ids_qcap)\n",
    "\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:, 0, :])\n",
    "            return logits\n",
    "\n",
    "        # Encode all evidence captions\n",
    "        caps_feature = []\n",
    "        for caption_list in caps:\n",
    "            encode_dict_cap = self.tokenizer(text=caption_list, max_length=128, truncation=True, padding='max_length')\n",
    "            input_ids_caps = paddle.to_tensor(encode_dict_cap['input_ids'])\n",
    "            cap_feature, _ = self.ernie(input_ids_caps)\n",
    "            cap_feature = cap_feature.mean(axis=1)  # mean pooling over all captions\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature, axis=0)\n",
    "\n",
    "        # Attention between qcap and caps\n",
    "        caps_feature = self.attention_text(qcap_feature, caps_feature, caps_feature)\n",
    "\n",
    "        # Attention over image features\n",
    "        # imgs_features = paddle.stack(imgs_features, axis=0)  # [B, N, 2048]\n",
    "        qImg_feature = qImg_feature.unsqueeze(1)  # [B, 1, 2048]\n",
    "        imgs_features = self.attention_image(qImg_feature, imgs_features, imgs_features)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        feature = paddle.concat(\n",
    "            [qcap_feature[:, 0, :], caps_feature[:, 0, :], qImg_feature.squeeze(1), imgs_features.squeeze(1)],\n",
    "            axis=-1)\n",
    "        logits = self.classifier1(feature)\n",
    "        logits = self.classifier2(logits)\n",
    "        return logits\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')  # 或 'weighted' 视情况而定\n",
    "    print(f\"Eval loss: {np.mean(losses):.5f}, acc: {acc:.5f}, f1: {f1:.5f}\")\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "\n",
    "# 声明模型\n",
    "model = NetWork(\"image\")\n",
    "#print(model)\n",
    "\n",
    "\n",
    "\n",
    "# AMP scaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "# 基本参数\n",
    "epochs = 10\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "print(f\"Total steps: {num_training_steps}\")\n",
    "\n",
    "# Cosine decay scheduler\n",
    "base_lr = 5e-5\n",
    "lr_scheduler = CosineAnnealingDecay(learning_rate=base_lr, T_max=num_training_steps)\n",
    "\n",
    "# 文件夹准备\n",
    "save_dir = \"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "# Decay params 设置\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 优化器 + gradient clipping\n",
    "optimizer = AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0),  # ⭐️ Gradient clipping ⭐️\n",
    "    apply_decay_param_fun=lambda x: x in decay_params\n",
    ")\n",
    "\n",
    "# Loss 和 Metric\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# 是否用 GPU\n",
    "if paddle.is_compiled_with_cuda():\n",
    "    paddle.set_device('gpu')\n",
    "else:\n",
    "    paddle.set_device('cpu')\n",
    "\n",
    "# 训练过程\n",
    "def do_train(model, criterion, metric, val_dataloader, train_dataloader, optimizer, lr_scheduler,\n",
    "             save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10):\n",
    "\n",
    "    global_step = 0\n",
    "    best_f1 = 0.0\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loader_progress = tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "        for step, batch in enumerate(train_loader_progress, start=1):\n",
    "            labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "\n",
    "            with paddle.amp.auto_cast():  # ⭐️ AMP 混合精度训练 ⭐️\n",
    "                logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                               caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            # Metric\n",
    "            correct = metric.compute(logits, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # AMP backward + optimizer step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            train_loader_progress.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{acc:.4f}\",\n",
    "                \"step\": global_step\n",
    "            })\n",
    "\n",
    "            # Eval\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n",
    "\n",
    "                if eval_f1 > best_f1:\n",
    "                    best_f1 = eval_f1\n",
    "                    best_model_path = os.path.join(best_dir, 'model_bestpro.pdparams')\n",
    "                    paddle.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"[BEST] Step {global_step} | F1: {eval_f1:.4f} (updated)\")\n",
    "\n",
    "# 训练启动\n",
    "do_train(\n",
    "    model, criterion, metric, val_dataloader, train_dataloader,\n",
    "    optimizer, lr_scheduler,\n",
    "    save_dir=\"checkpoint\", best_dir=\"best_model\", epochs=10\n",
    ")\n",
    "\n",
    "# 加载最优模型 + 测试\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = os.path.join(\"best_model\", \"model_bestpro.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "    caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "    logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                   caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "    preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1).numpy()\n",
    "    results.extend(preds.tolist())\n",
    "\n",
    "# 保存\n",
    "pd.DataFrame({\"id\": range(len(results)), \"label\": results}).to_csv(\"result.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15c3c05-9e8c-486f-a076-0ff2e710529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 655/655 [01:38<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.14575, Accuracy: 0.76929, F1 Score (macro): 0.75322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 验证函数（带F1）\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        labels, caps_batch, imgs_feature_batch, qCap_batch, qImg_feature_batch = batch\n",
    "        logits = model(qCap=qCap_batch, qImg_feature=qImg_feature_batch,\n",
    "                       caps=caps_batch, imgs_features=imgs_feature_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        preds = paddle.argmax(F.softmax(logits, axis=-1), axis=1)\n",
    "        all_preds.extend(preds.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "        gc.collect()\n",
    "        paddle.device.cuda.empty_cache()\n",
    "\n",
    "    acc = metric.accumulate()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(\"Eval Loss: {:.5f}, Accuracy: {:.5f}, F1 Score (macro): {:.5f}\".format(np.mean(losses), acc, f1))\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "# 加载模型参数\n",
    "params_path = os.path.join(\"best_model\", \"model_bestpro.pdparams\")\n",
    "if os.path.exists(params_path):\n",
    "    model.set_dict(paddle.load(params_path))\n",
    "    print(\"Loaded best model.\")\n",
    "else:\n",
    "    print(\"Best model file not found!\")\n",
    "\n",
    "# 损失函数和评估指标\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# 执行评估\n",
    "eval_loss, eval_acc, eval_f1 = evaluate(model, criterion, metric, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11677d-1d31-43ea-ab2c-2cee0799d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle1",
   "language": "python",
   "name": "paddle1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
